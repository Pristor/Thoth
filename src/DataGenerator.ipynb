{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import data_source.preproc as pp\n",
    "import h5py\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class DataGenerator():\n",
    "    \n",
    "    def __init__(self, source, batchsize, maxTextLenght, predict = False):\n",
    "        \"\"\"DataGenerator class, functions: next_train_batch\n",
    "                                           next_valid_batch\n",
    "                                           next_test_batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.charset = string.printable[:95] #All possible chars that the model will predict\n",
    "        self.maxTextLenght = maxTextLenght\n",
    "        \n",
    "        self.tokenizer = Tokenizer(self.charset, self.maxTextLenght)\n",
    "        \n",
    "        self.batchsize = batchsize\n",
    "        self.partitions = ['test'] if predict else ['train', 'valid', 'test']\n",
    "        \n",
    "        self.size = dict()\n",
    "        self.steps = dict()\n",
    "        self.index = dict()\n",
    "        self.dataset = dict()\n",
    "        \n",
    "        with h5py.File(source, \"r\") as f:\n",
    "            for pt in self.partitions:\n",
    "                self.dataset[pt] = dict()\n",
    "                self.dataset[pt]['dt'] = f[pt]['dt'][:]\n",
    "                self.dataset[pt]['gt'] = f[pt]['gt'][:]\n",
    "                \n",
    "        for pt in self.partitions:\n",
    "            # decode sentences from byte\n",
    "            self.dataset[pt]['gt'] = [x.decode() for x in self.dataset[pt]['gt']]\n",
    "\n",
    "            # set size and setps\n",
    "            self.size[pt] = len(self.dataset[pt]['gt'])\n",
    "            self.steps[pt] = int(np.ceil(self.size[pt] / self.batchsize))\n",
    "            self.index[pt] = 0\n",
    "            \n",
    "            \n",
    "        \n",
    "    def next_train_batch(self):\n",
    "        \"get the next batch, function yields batch\"\n",
    "        \n",
    "        while(True):\n",
    "            if self.index['train'] >= self.size[\"train\"]:\n",
    "                #reset index if all trainings example have been taken\n",
    "                self.index['train'] = 0\n",
    "            \n",
    "            #index -> index + batchsize and index -> batchsize\n",
    "            index = self.index['train']\n",
    "            until = index + self.batchsize\n",
    "            self.index['train'] = until\n",
    "            \n",
    "            x_train = self.dataset['train']['dt'][index:until]\n",
    "            y_train = self.dataset['train']['gt'][index:until]\n",
    "            \n",
    "            \n",
    "            #Augment trainings data:\n",
    "            x_train = pp.augmentation(x_train,\n",
    "                                      rotation_range=1.5,\n",
    "                                      scale_range=0.05,\n",
    "                                      height_shift_range=0.025,\n",
    "                                      width_shift_range=0.05,\n",
    "                                      erode_range=5,\n",
    "                                      dilate_range=3)\n",
    "            \n",
    "            x_train = pp.normalization(x_train)\n",
    "            \n",
    "            y_train = [self.tokenizer.encode(i) for i in y_train]\n",
    "            y_train = pad_sequences(y_train, maxlen=self.tokenizer.maxlen, padding=\"post\")\n",
    "            \n",
    "            yield(x_train, y_train, [])\n",
    "            \n",
    "            \n",
    "    def next_valid_batch(self):\n",
    "        \"get the next validation batch, function yields the batch\"\n",
    "        \n",
    "        while(True):\n",
    "            if self.index['valid'] >= self.size['valid']:\n",
    "                 self.index['valid'] = 0\n",
    "                \n",
    "            index = self.index['valid']\n",
    "            until = index + self.batchsize\n",
    "            self.index['valid'] = until\n",
    "                \n",
    "            x_valid = self.dataset['valid']['dt'][index:until]\n",
    "            y_valid = self.dataset['valid']['gt'][index:until]\n",
    "                \n",
    "            x_valid = pp.normalization(x_valid)\n",
    "                \n",
    "            y_valid = [self.tokenizer.encode(i) for i in y_valid]\n",
    "            y_valid = pad_sequences(y_valid, maxlen=self.tokenizer.maxlen, padding=\"post\")\n",
    "            \n",
    "            yield (x_valid, y_valid, [])\n",
    "                \n",
    "                \n",
    "        \n",
    "    def next_test_batch(self):\n",
    "        \n",
    "        while(True):\n",
    "            if self.index['test'] >= self.size['test']:\n",
    "                self.index['test'] = 0\n",
    "                \n",
    "            index = self.index['test']\n",
    "            until = index + self.batchsize\n",
    "            self.index['test'] = until\n",
    "                \n",
    "            x_test = self.dataset['test']['dt'][index:until]\n",
    "            x_test = pp.normalization(x_test)\n",
    "                \n",
    "            yield x_test\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \n",
    "    def __init__(self, chars, max_TextLenght):\n",
    "        \"\"\"Tokenizerclass Functions: encode() char -> numpy vector\n",
    "                                     decode() numpy vector -> chars\n",
    "                                     remove_tokens() removes PAD token from text\n",
    "        \"\"\"\n",
    "        self.PAD_TK, self.UNK_TK = \"¶\", \"¤\"\n",
    "        self.chars = (self.PAD_TK + self.UNK_TK + chars)\n",
    "\n",
    "        self.PAD = self.chars.find(self.PAD_TK)\n",
    "        self.UNK = self.chars.find(self.UNK_TK)\n",
    "\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.maxlen = max_TextLenght\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"encode data into Vector char -> index\"\n",
    "        \n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "        text = \" \".join(text.split())\n",
    "        #self.test = 0\n",
    "        \n",
    "        encoded = []\n",
    "        \n",
    "        for item in text:\n",
    "            #get a Vector with a number from 0 to len(chars), each letter gets a number\n",
    "            index = self.chars.find(item)\n",
    "            index = self.UNK if index == -1 else index\n",
    "            encoded.append(index)\n",
    "            #if self.test == 10:\n",
    "                #print(encoded)\n",
    "            #self.test = self.test + 1\n",
    "            \n",
    "\n",
    "        return np.asarray(encoded)\n",
    "    \n",
    "    def decode(self, text):\n",
    "        \"\"\"Decode vector to text\"\"\"\n",
    "\n",
    "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
    "        decoded = self.remove_tokens(decoded)\n",
    "        decoded = pp.text_standardize(decoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def remove_tokens(self, text):\n",
    "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
    "\n",
    "        return text.replace(self.PAD_TK, \"\")\n",
    "        \n",
    "    \n",
    "    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
