{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.layers import Reshape, Bidirectional, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTRModel:\n",
    "    \n",
    "    def __init__(self, input_size, vocab_size, greedy = False, beam_width=10, top_paths=1):\n",
    "        \"\"\"\n",
    "        init model, params: input_size of data, vocab_size: ammount of possible chars\n",
    "        greedy, beam_width and top_patchs for ctc_encoding\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.model = None\n",
    "        \n",
    "        self.greedy = greedy\n",
    "        self.beam_width = beam_width\n",
    "        self.top_paths = max(1 ,top_paths)\n",
    "        \n",
    "        \n",
    "    def compile(self, learning_rate):\n",
    "        \n",
    "        inputs, outputs, optimizer = self.prepNN(learning_rate=learning_rate, vocabsize=(self.vocab_size + 1))\n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer=optimizer, loss=self.ctc_loss_lambda_func)\n",
    "        \n",
    "    \n",
    "    def prepNN(self, learning_rate, vocabsize):\n",
    "        \n",
    "        input_data = Input(name=\"input\", shape=self.input_size)\n",
    "\n",
    "        cnn = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(input_data)\n",
    "        cnn = BatchNormalization()(cnn)\n",
    "        cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "        cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
    "\n",
    "        cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
    "        cnn = BatchNormalization()(cnn)\n",
    "        cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "        cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
    "\n",
    "        cnn = Dropout(rate=0.2)(cnn)\n",
    "        cnn = Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
    "        cnn = BatchNormalization()(cnn)\n",
    "        cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "        cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
    "\n",
    "        cnn = Dropout(rate=0.2)(cnn)\n",
    "        cnn = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
    "        cnn = BatchNormalization()(cnn)\n",
    "        cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "\n",
    "        cnn = Dropout(rate=0.2)(cnn)\n",
    "        cnn = Conv2D(filters=80, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
    "        cnn = BatchNormalization()(cnn)\n",
    "        cnn = LeakyReLU(alpha=0.01)(cnn)\n",
    "\n",
    "        shape = cnn.get_shape()\n",
    "        blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
    "\n",
    "        blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "        blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "        blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "        blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "        blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
    "\n",
    "        blstm = Dropout(rate=0.5)(blstm)\n",
    "        output_data = Dense(units=vocabsize, activation=\"softmax\")(blstm)\n",
    "\n",
    "        if learning_rate is None:\n",
    "            learning_rate = 3e-4\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "        return (input_data, output_data, optimizer)\n",
    "        \n",
    "    \n",
    "    def fit(self, \n",
    "            x=None, \n",
    "            y=None, \n",
    "            batch_size= None, \n",
    "            epochs = 1, \n",
    "            verbose=1,\n",
    "            callbacks=None,\n",
    "            validation_split=0.0,\n",
    "            validation_data=None,\n",
    "            shuffle=True,\n",
    "            class_weight=None,\n",
    "            sample_weight=None,\n",
    "            initial_epoch=0,\n",
    "            steps_per_epoch=None,\n",
    "            validation_steps=None,\n",
    "            validation_freq=1,\n",
    "            max_queue_size=10,\n",
    "            workers=1,\n",
    "            use_multiprocessing=False,\n",
    "            **kwargs):\n",
    "        \n",
    "        out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
    "                             callbacks=callbacks, validation_split=validation_split,\n",
    "                             validation_data=validation_data, shuffle=shuffle,\n",
    "                             class_weight=class_weight, sample_weight=sample_weight,\n",
    "                             initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch,\n",
    "                             validation_steps=validation_steps, validation_freq=validation_freq,\n",
    "                             max_queue_size=max_queue_size, workers=workers,\n",
    "                             use_multiprocessing=False, **kwargs)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def predict(self,\n",
    "                x,\n",
    "                batch_size=None,\n",
    "                verbose=0,\n",
    "                steps=1,\n",
    "                callbacks=None,\n",
    "                max_queue_size=10,\n",
    "                workers=1,\n",
    "                use_multiprocessing=False,\n",
    "                ctc_decode=True):\n",
    "        \"\"\"\n",
    "        Model predicting on data yielded (predict function has support to generator).\n",
    "        A predict() abstration function of TensorFlow 2.\n",
    "\n",
    "        Provide x parameter of the form: yielding [x].\n",
    "\n",
    "        :param: See tensorflow.keras.Model.predict()\n",
    "        :return: raw data on `ctc_decode=False` or CTC decode on `ctc_decode=True` (both with probabilities)\n",
    "        \"\"\"\n",
    "\n",
    "        self.model._make_predict_function()\n",
    "\n",
    "        if verbose == 1:\n",
    "            print(\"Model Predict\")\n",
    "\n",
    "        out = self.model.predict(x=x, batch_size=batch_size, verbose=verbose, steps=steps,\n",
    "                                 callbacks=callbacks, max_queue_size=max_queue_size,\n",
    "                                 workers=workers, use_multiprocessing=use_multiprocessing)\n",
    "\n",
    "        if not ctc_decode:\n",
    "            return np.log(out.clip(min=1e-8))\n",
    "\n",
    "        steps_done = 0\n",
    "        if verbose == 1:\n",
    "            print(\"CTC Decode\")\n",
    "            progbar = tf.keras.utils.Progbar(target=steps)\n",
    "\n",
    "        batch_size = int(np.ceil(len(out) / steps))\n",
    "        input_length = len(max(out, key=len))\n",
    "\n",
    "        predicts, probabilities = [], []\n",
    "\n",
    "        while steps_done < steps:\n",
    "            index = steps_done * batch_size\n",
    "            until = index + batch_size\n",
    "\n",
    "            x_test = np.asarray(out[index:until])\n",
    "            x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
    "\n",
    "            decode, log = K.ctc_decode(x_test,\n",
    "                                       x_test_len,\n",
    "                                       greedy=self.greedy,\n",
    "                                       beam_width=self.beam_width,\n",
    "                                       top_paths=self.top_paths)\n",
    "\n",
    "            probabilities.extend([np.exp(x) for x in log])\n",
    "            decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
    "            predicts.extend(np.swapaxes(decode, 0, 1))\n",
    "\n",
    "            steps_done += 1\n",
    "            if verbose == 1:\n",
    "                progbar.update(steps_done)\n",
    "\n",
    "        return (predicts, probabilities)\n",
    "    \n",
    "        \n",
    "    def summary(self, output=None, target=None):\n",
    "        \"\"\"Show/Save model structure (summary)\"\"\"\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "        if target is not None:\n",
    "            os.makedirs(output, exist_ok=True)\n",
    "\n",
    "            with open(os.path.join(output, target), \"w\") as f:\n",
    "                with redirect_stdout(f):\n",
    "                    self.model.summary()\n",
    "    \n",
    "    def get_callbacks(self, logdir, checkpoint, monitor=\"val_loss\", verbose=0):\n",
    "        \"\"\"Setup the list of callbacks for the model\"\"\"\n",
    "\n",
    "        callbacks = [\n",
    "            CSVLogger(\n",
    "                filename=os.path.join(logdir, \"epochs.log\"),\n",
    "                separator=\";\",\n",
    "                append=True),\n",
    "            TensorBoard(\n",
    "                log_dir=logdir,\n",
    "                histogram_freq=10,\n",
    "                profile_batch=0,\n",
    "                write_graph=True,\n",
    "                write_images=False,\n",
    "                update_freq=\"epoch\"),\n",
    "            ModelCheckpoint(\n",
    "                filepath=checkpoint,\n",
    "                monitor=monitor,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                verbose=verbose),\n",
    "            EarlyStopping(\n",
    "                monitor=monitor,\n",
    "                min_delta=1e-8,\n",
    "                patience=20,\n",
    "                restore_best_weights=True,\n",
    "                verbose=verbose),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor=monitor,\n",
    "                min_delta=1e-8,\n",
    "                factor=0.2,\n",
    "                patience=15,\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        return callbacks\n",
    "    \n",
    "    def load_checkpoint(self, target):\n",
    "        \"\"\" Load a model with checkpoint file\"\"\"\n",
    "\n",
    "        if os.path.isfile(target):\n",
    "            if self.model is None:\n",
    "                self.compile()\n",
    "\n",
    "            self.model.load_weights(target)\n",
    "        \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def ctc_loss_lambda_func(y_true, y_pred):\n",
    "        \"\"\"Function for computing the CTC loss\"\"\"\n",
    "\n",
    "        if len(y_true.shape) > 2:\n",
    "            y_true = tf.squeeze(y_true)\n",
    "\n",
    "        # y_pred.shape = (batch_size, string_length, alphabet_size_1_hot_encoded)\n",
    "        # output of every model is softmax\n",
    "        # so sum across alphabet_size_1_hot_encoded give 1\n",
    "        #               string_length give string length\n",
    "        input_length = tf.math.reduce_sum(y_pred, axis=-1, keepdims=False)\n",
    "        input_length = tf.math.reduce_sum(input_length, axis=-1, keepdims=True)\n",
    "\n",
    "        # y_true strings are padded with 0\n",
    "        # so sum of non-zero gives number of characters in this string\n",
    "        label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True, dtype=\"int64\")\n",
    "\n",
    "        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "\n",
    "        # average loss across all entries in the batch\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
